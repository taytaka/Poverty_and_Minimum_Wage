{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_EDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Bkcdl6499-"
      },
      "source": [
        "# PySpark ETL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBs_DzN05IVr"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.2'\n",
        "spark_version = 'spark-3.1.2'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDWEGw_D5I59"
      },
      "source": [
        "# Download Postgres driver\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvtpM6Pu5Lc3"
      },
      "source": [
        "# Import dependencies\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "import sklearn as skl\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from pyspark import SparkFiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33lgzoZ-5cP6"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbAvjWCL5Qe8"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR1gBu125Sll"
      },
      "source": [
        "# Import and read our input CSV dataset\n",
        "df = pd.read_csv('<file_name.csv>')\n",
        "df.head()\n",
        "\n",
        "# OR Read in data from S3 Buckets into a DataFrame\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://YOUR-BUCKET-NAME.s3.amazonaws.com/<file_name.csv\">\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"<file_name.csv), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# This is our db url = \"database-1.czpjmlarn3xk.us-east-2.rds.amazonaws.com\"\n",
        "# Be sure to change the S3 bucket name to your bucket name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDMiAuZ05Ufn"
      },
      "source": [
        "## Extract\n",
        "Connect to database and then extract data into a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seq3QA9X5k05"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://YOUR-BUCKET-NAME.s3.amazonaws.com/<file_name.csv>\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"<file_name.csv>\"), sep=\",\", header=True, inferSchema=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpJgvNP-5N7t"
      },
      "source": [
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2McvfOC5w5b"
      },
      "source": [
        "## Transform\n",
        "Transformm raw data stored in S3 in PySpark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBr4tvuf56lX"
      },
      "source": [
        "# Drop any roles with null or \"not a number\" (NaN) values\n",
        "dropna_df = joined_df.dropna()\n",
        "dropna_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BHsG59h5vYL"
      },
      "source": [
        "# Drop the non-beneficial columns\n",
        "df = df.drop(columns=[\"\", \"N\"], axis=1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feIcJ_Yv6EvF"
      },
      "source": [
        "# Determine the number of unique values in each column\n",
        "cnt = df.nunique(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMF4ElLb6Gwx"
      },
      "source": [
        "# Check for unique values is to use the Pandas DataFrame's value_counts method\n",
        "application_counts = df.column_name.value_counts()\n",
        "application_counts "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nN0yKpZ6RG0"
      },
      "source": [
        "# Visualize the value counts of APPLICATION_TYPE\n",
        "application_counts.plot.density()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM7uCfVD6TcZ"
      },
      "source": [
        "# Determine which values to replace if counts are less than ...?\n",
        "replace_application = list(application_counts[application_counts < 500].index)\n",
        "\n",
        "# Replace in dataframe\n",
        "for app in replace_application:\n",
        "    application_df.column_name = application_df.column_name.replace(app,\"Other\")\n",
        "    \n",
        "# Check to make sure binning was successful\n",
        "application_df.column_name.value_counts()\n",
        "\n",
        "# This reduces the number of unique values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp2tl25W6Z-y"
      },
      "source": [
        "# Generate our categorical variable lists\n",
        "application_cat = df.dtypes[df.dtypes == \"object\"].index.tolist()\n",
        "application_cat\n",
        "\n",
        "# Generate categorical list prior to encoding all categorical data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um8S2eM_6iEV"
      },
      "source": [
        "# Create a OneHotEncoder instance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the OneHotEncoder using the categorical variable list\n",
        "encode_df = pd.DataFrame(enc.fit_transform(df[application_cat]))\n",
        "\n",
        "# Add the encoded variable names to the dataframe (rename encoded column)\n",
        "encode_df.columns = enc.get_feature_names(application_cat)\n",
        "encode_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph0DXJ8k6iuH"
      },
      "source": [
        "# Merge one-hot encoded features and drop the originals\n",
        "merged_df = df.merge(encode_df,left_index=True,right_index=True).drop(application_cat,1)\n",
        "merged_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8ePbPiH6k-p"
      },
      "source": [
        "## Load transformed raw data into our database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x9tnTXA6rFQ"
      },
      "source": [
        "# Store environmental variable\n",
        "from getpass import getpass\n",
        "password = getpass('Enter database password')\n",
        "# Configure settings for RDS\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://<connection string>:5432/<database-name>\"\n",
        "config = {\"user\":\"postgres\",\n",
        "          \"password\": password,\n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHjQMPpF61kq"
      },
      "source": [
        "## Write the cleaned DataFrame directly into database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bim7CbzK69W0"
      },
      "source": [
        "# Write DataFrame to active_user table in RDS\n",
        "merged_df.write.jdbc(url=jdbc_url, table='<table_name>', mode=mode, properties=config)\n",
        "\n",
        "# If we are loading into multiple tables, create separate DataFrames to matcch the table\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s2l_KYx7ZzZ"
      },
      "source": [
        "## Validate data successfully written to database by running queries in pgAdmin on the database"
      ]
    }
  ]
}