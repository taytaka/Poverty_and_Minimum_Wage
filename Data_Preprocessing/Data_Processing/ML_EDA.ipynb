{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Bkcdl6499-"
   },
   "source": [
    "# PySpark ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBs_DzN05IVr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.2'\n",
    "spark_version = 'spark-3.1.2'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDWEGw_D5I59"
   },
   "outputs": [],
   "source": [
    "# Download Postgres driver\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvtpM6Pu5Lc3"
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import sklearn as skl\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33lgzoZ-5cP6"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbAvjWCL5Qe8"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SR1gBu125Sll"
   },
   "outputs": [],
   "source": [
    "# Import and read our input CSV dataset\n",
    "df = pd.read_csv('<file_name.csv>')\n",
    "df.head()\n",
    "\n",
    "# OR Read in data from S3 Buckets into a DataFrame\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://YOUR-BUCKET-NAME.s3.amazonaws.com/<file_name.csv\">\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"<file_name.csv), sep=\",\", header=True, inferSchema=True)\n",
    "\n",
    "# This is our db url = \"database-1.czpjmlarn3xk.us-east-2.rds.amazonaws.com\"\n",
    "# Be sure to change the S3 bucket name to your bucket name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDMiAuZ05Ufn"
   },
   "source": [
    "## Extract\n",
    "Connect to database and then extract data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seq3QA9X5k05"
   },
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://YOUR-BUCKET-NAME.s3.amazonaws.com/<file_name.csv>\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"<file_name.csv>\"), sep=\",\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpJgvNP-5N7t"
   },
   "outputs": [],
   "source": [
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2McvfOC5w5b"
   },
   "source": [
    "## Transform\n",
    "Transformm raw data stored in S3 in PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBr4tvuf56lX"
   },
   "outputs": [],
   "source": [
    "# Drop any roles with null or \"not a number\" (NaN) values\n",
    "dropna_df = joined_df.dropna()\n",
    "dropna_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BHsG59h5vYL"
   },
   "outputs": [],
   "source": [
    "# Drop the non-beneficial columns\n",
    "df = df.drop(columns=[\"\", \"N\"], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feIcJ_Yv6EvF"
   },
   "outputs": [],
   "source": [
    "# Determine the number of unique values in each column\n",
    "cnt = df.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMF4ElLb6Gwx"
   },
   "outputs": [],
   "source": [
    "# Check for unique values is to use the Pandas DataFrame's value_counts method\n",
    "application_counts = df.column_name.value_counts()\n",
    "application_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nN0yKpZ6RG0"
   },
   "outputs": [],
   "source": [
    "# Visualize the value counts of APPLICATION_TYPE\n",
    "application_counts.plot.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM7uCfVD6TcZ"
   },
   "outputs": [],
   "source": [
    "# Determine which values to replace if counts are less than ...?\n",
    "replace_application = list(application_counts[application_counts < 500].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in replace_application:\n",
    "    application_df.column_name = application_df.column_name.replace(app,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "application_df.column_name.value_counts()\n",
    "\n",
    "# This reduces the number of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp2tl25W6Z-y"
   },
   "outputs": [],
   "source": [
    "# Generate our categorical variable lists\n",
    "application_cat = df.dtypes[df.dtypes == \"object\"].index.tolist()\n",
    "application_cat\n",
    "\n",
    "# Generate categorical list prior to encoding all categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Um8S2eM_6iEV"
   },
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(df[application_cat]))\n",
    "\n",
    "# Add the encoded variable names to the dataframe (rename encoded column)\n",
    "encode_df.columns = enc.get_feature_names(application_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ph0DXJ8k6iuH"
   },
   "outputs": [],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "merged_df = df.merge(encode_df,left_index=True,right_index=True).drop(application_cat,1)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8ePbPiH6k-p"
   },
   "source": [
    "## Load transformed raw data into our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7x9tnTXA6rFQ"
   },
   "outputs": [],
   "source": [
    "# Store environmental variable\n",
    "from getpass import getpass\n",
    "password = getpass('Enter database password')\n",
    "# Configure settings for RDS\n",
    "mode = \"append\"\n",
    "jdbc_url=\"jdbc:postgresql://<connection string>:5432/<database-name>\"\n",
    "config = {\"user\":\"postgres\",\n",
    "          \"password\": password,\n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHjQMPpF61kq"
   },
   "source": [
    "## Write the cleaned DataFrame directly into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bim7CbzK69W0"
   },
   "outputs": [],
   "source": [
    "# Write DataFrame to active_user table in RDS\n",
    "merged_df.write.jdbc(url=jdbc_url, table='<table_name>', mode=mode, properties=config)\n",
    "\n",
    "# If we are loading into multiple tables, create separate DataFrames to matcch the table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6s2l_KYx7ZzZ"
   },
   "source": [
    "## Validate data successfully written to database by running queries in pgAdmin on the database"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_EDA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
